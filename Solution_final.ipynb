{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c11b8c6477e96bdd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassNamePrefixFeaturesOutMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.utils import compute_class_weight\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from xgboost.callback import TrainingCallback\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cdc58b0f82f49a4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0c22bd5d850548d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a85ae717ae7eacb4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "DATA_DIR = 'data'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7241a362671b3577",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = pd.read_csv(f'{DATA_DIR}/test_values.csv')\n",
    "train_x = pd.read_csv(f'{DATA_DIR}/train_values.csv')\n",
    "train_y = pd.read_csv(f'{DATA_DIR}/train_labels.csv')\n",
    "train_y['damage_grade'] -= 1  # Adjust y"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "geo_level_columns = ['geo_level_1_id', 'geo_level_2_id', 'geo_level_3_id']\n",
    "categorical_columns = ['foundation_type', 'ground_floor_type', 'land_surface_condition',\n",
    "                       'legal_ownership_status', 'other_floor_type',\n",
    "                       'plan_configuration', 'position', 'roof_type']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76566ece9e717efe",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autoencoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbf95e2946b4303f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "geo = pd.concat([train_x[geo_level_columns], test_x[geo_level_columns]])\n",
    "label_encoder_1 = LabelEncoder()\n",
    "label_encoder_2 = LabelEncoder()\n",
    "label_encoder_3 = LabelEncoder()\n",
    "geo[\"geo_level_1_id\"] = label_encoder_1.fit_transform(geo[\"geo_level_1_id\"])\n",
    "geo[\"geo_level_2_id\"] = label_encoder_2.fit_transform(geo[\"geo_level_2_id\"])\n",
    "geo[\"geo_level_3_id\"] = label_encoder_3.fit_transform(geo[\"geo_level_3_id\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9f4a101fbf8225",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, lv1_shape, lv2_shape, lv3_shape, dim_1, dim_2, dim_3, enc_shape):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.encode_lv1 = torch.nn.Embedding(lv1_shape, dim_1)\n",
    "        self.encode_lv2 = torch.nn.Embedding(lv2_shape, dim_2)\n",
    "        self.encode_lv3 = torch.nn.Embedding(lv3_shape, dim_3)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.encoder = torch.nn.Linear(dim_1 + dim_2 + dim_3, enc_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.encode_lv1(x[:, 0])\n",
    "        x2 = self.encode_lv2(x[:, 1])\n",
    "        x3 = self.encode_lv3(x[:, 2])\n",
    "        x = torch.concat((x1, x2, x3), dim=1)\n",
    "        x = self.relu(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, lv1_shape, lv2_shape, lv3_shape, enc_shape):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.decode_lv1 = torch.nn.Linear(enc_shape, lv1_shape)\n",
    "        self.decode_lv2 = torch.nn.Linear(enc_shape, lv2_shape)\n",
    "        self.decode_lv3 = torch.nn.Linear(enc_shape, lv3_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.decode_lv1(x)\n",
    "        x2 = self.decode_lv2(x)\n",
    "        x3 = self.decode_lv3(x)\n",
    "\n",
    "        return x1, x2, x3\n",
    "\n",
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, lv1_shape, lv2_shape, lv3_shape, enc_shape, dim_1, dim_2, dim_3):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            lv1_shape=lv1_shape,\n",
    "            lv2_shape=lv2_shape,\n",
    "            lv3_shape=lv3_shape,\n",
    "            enc_shape=enc_shape,\n",
    "            dim_1=dim_1,\n",
    "            dim_2=dim_2,\n",
    "            dim_3=dim_3\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            lv1_shape=lv1_shape,\n",
    "            lv2_shape=lv2_shape,\n",
    "            lv3_shape=lv3_shape,\n",
    "            enc_shape=enc_shape\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a6f5a168fcd85f58",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e99217c216c586c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prepare dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4115ecd5feeb666"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = TensorDataset(\n",
    "    torch.from_numpy(np.array(geo)).type(torch.long),\n",
    "    torch.from_numpy(np.array(geo)).type(torch.long)\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=128\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a00013c791b663c5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define criterions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "739821b717442730"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "geo_lv1_weights = compute_class_weight('balanced', classes=geo['geo_level_1_id'].unique(),\n",
    "                                       y=geo['geo_level_1_id'].values)\n",
    "geo_lv2_weights = compute_class_weight('balanced', classes=geo['geo_level_2_id'].unique(),\n",
    "                                       y=geo['geo_level_2_id'].values)\n",
    "geo_lv3_weights = compute_class_weight('balanced', classes=geo['geo_level_3_id'].unique(),\n",
    "                                       y=geo['geo_level_3_id'].values)\n",
    "\n",
    "criterion_1 = torch.nn.CrossEntropyLoss(torch.from_numpy(geo_lv1_weights).type(torch.float).to(DEVICE))\n",
    "criterion_2 = torch.nn.CrossEntropyLoss(torch.from_numpy(geo_lv2_weights).type(torch.float).to(DEVICE))\n",
    "criterion_3 = torch.nn.CrossEntropyLoss(torch.from_numpy(geo_lv3_weights).type(torch.float).to(DEVICE))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8cd977aa26be0ef",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4af3f2c243ea4bf8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = Autoencoder(\n",
    "    lv1_shape=geo['geo_level_1_id'].nunique(),\n",
    "    lv2_shape=geo['geo_level_2_id'].nunique(),\n",
    "    lv3_shape=geo['geo_level_3_id'].nunique(),\n",
    "    enc_shape=16,\n",
    "    dim_1=8,\n",
    "    dim_2=16,\n",
    "    dim_3=32\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "\n",
    "model.train()\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "num_epochs = 10000\n",
    "patience = 50\n",
    "epochs_no_improvement = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'EPOCH: {epoch + 1}')\n",
    "    training_loss = 0\n",
    "    for x, y in tqdm(dataloader, desc=\"training\"):\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x1, x2, x3 = model(x)\n",
    "\n",
    "        loss = criterion_1(x1, y[:, 0]) + criterion_2(x2, y[:, 1]) + criterion_3(x3, y[:, 2])\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        training_loss += loss.item()\n",
    "\n",
    "    training_loss /= len(dataloader.dataset)\n",
    "    print(f'{training_loss = }')\n",
    "\n",
    "    scheduler.step(training_loss)\n",
    "\n",
    "    if best_loss > training_loss:\n",
    "        best_loss = training_loss\n",
    "        best_model = model\n",
    "        epochs_no_improvement = 0\n",
    "    else:\n",
    "        epochs_no_improvement += 1\n",
    "        if epochs_no_improvement >= patience:\n",
    "            print(f'{patience} epochs without improvement, stopping training.')\n",
    "            break\n",
    "\n",
    "print(f'Training complete. Best loss: {best_loss}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e85a8acf6737439",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c322df0507dc291"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "joblib.dump(best_model.encoder, 'encoder_weights_81632.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5f42015c062aa58",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fe45935c3074054"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "encoder = joblib.load('encoder_weights_81632.pkl')\n",
    "encoder.to(DEVICE)\n",
    "encoder.eval();"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee391c537b8d0af4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class RemoveLowCountCategories(BaseEstimator, TransformerMixin, ClassNamePrefixFeaturesOutMixin):\n",
    "\n",
    "    def __init__(self, threshold, replace_value):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.replace_value = replace_value\n",
    "        self.keep_dict = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "        self.keep_dict = {\n",
    "            column: X[column].value_counts().index[(X[column].value_counts() > self.threshold)] for column in X.columns\n",
    "        }\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        assert self.keep_dict is not None\n",
    "\n",
    "        out = X.copy()\n",
    "\n",
    "        for column, keep_categories in self.keep_dict.items():\n",
    "            to_replace = set(out[column].unique()).difference(set(keep_categories))\n",
    "            out[column].replace(to_replace, self.replace_value, inplace=True)\n",
    "\n",
    "        # Replace geo_id_3 with -1 if geo_id_2 is -1\n",
    "        out['geo_level_3_id'] = out.apply(\n",
    "            lambda x: -1 if x['geo_level_2_id'] == -1 else x['geo_level_3_id'],\n",
    "            axis=1)\n",
    "\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2384b0e9077d68ab",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get geo level columns\n",
    "geo_train = train_x[geo_level_columns]\n",
    "\n",
    "# Label encoding\n",
    "geo_train.loc[:, \"geo_level_1_id\"] = label_encoder_1.transform(geo_train[\"geo_level_1_id\"])\n",
    "geo_train.loc[:, \"geo_level_2_id\"] = label_encoder_2.transform(geo_train[\"geo_level_2_id\"])\n",
    "geo_train.loc[:, \"geo_level_3_id\"] = label_encoder_3.transform(geo_train[\"geo_level_3_id\"])\n",
    "\n",
    "# Auto encoding\n",
    "geo_train_tensor = torch.from_numpy(np.array(geo_train)).type(torch.long).to(DEVICE)\n",
    "geo_train = pd.DataFrame(encoder(geo_train_tensor).detach().cpu().numpy())\n",
    "train_x_modified = pd.concat([train_x, geo_train], axis=1)\n",
    "\n",
    "# Fix columns type\n",
    "train_x_modified.columns = train_x_modified.columns.astype(str)\n",
    "\n",
    "# Remove low count geo id\n",
    "remove_low_count = RemoveLowCountCategories(3, -1)\n",
    "remove_low_count.fit(geo)\n",
    "train_x_modified.loc[:, geo_level_columns] = remove_low_count.transform(train_x_modified[geo_level_columns])\n",
    "\n",
    "# One-hot encoding\n",
    "one_hot_encoder = OneHotEncoder(drop='first', handle_unknown='ignore', min_frequency=1, sparse_output=False)\n",
    "encoded_categorical = one_hot_encoder.fit_transform(train_x_modified[categorical_columns])\n",
    "encoded_categorical = pd.DataFrame(\n",
    "    encoded_categorical,\n",
    "    columns=one_hot_encoder.get_feature_names_out(categorical_columns)\n",
    ")\n",
    "train_x_modified.drop(categorical_columns, axis=1, inplace=True)\n",
    "train_x_modified = pd.concat([train_x_modified, encoded_categorical], axis=1)\n",
    "\n",
    "# Drop building id\n",
    "train_x_modified.drop('building_id', axis=1, inplace=True)\n",
    "train_y_modified = train_y.drop('building_id', axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cbc3211a73bc69",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfaf0697c2234f4c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Split train test\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x_modified, train_y_modified, test_size=0.3)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'multi:softmax',\n",
    "        'num_class': 3,\n",
    "        'eval_metric': 'auc',\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'max_depth': trial.suggest_int('max_depth', 10, 13),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 800, 1000),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 0.9),\n",
    "        'eta': trial.suggest_float('eta', 0.01, 0.1),\n",
    "        'gamma': trial.suggest_float('gamma', 0.7, 1.0),\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 5, 7),\n",
    "        'random_state': 37\n",
    "    }\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(**params)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average='micro')\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, n_jobs=3)\n",
    "\n",
    "print(f\"Best trial: {study.best_trial.value}\")\n",
    "print(f\"Best parameters: {study.best_trial.params}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "683a26117b0ddafb",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3d21bcd0ac81768"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 3,\n",
    "    'eval_metric': 'auc',\n",
    "    'booster': 'gbtree',\n",
    "    'tree_method': 'gpu_hist',\n",
    "    'max_depth': 12,\n",
    "    'subsample': 0.7134570758579321,\n",
    "    'n_estimators': 958,\n",
    "    'colsample_bytree': 0.7145579796503638,\n",
    "    'eta': 0.024867991827546986,\n",
    "    'gamma': 0.8898009647421944,\n",
    "    'min_child_weight': 6.13488658574256\n",
    "}\n",
    "\n",
    "\n",
    "class TqdmCallback(TrainingCallback):\n",
    "    def __init__(self, total):\n",
    "        super().__init__()\n",
    "        self.pbar = tqdm(total=total, desc=\"Training\")\n",
    "\n",
    "    def after_iteration(self, model, epoch, evals_log):\n",
    "        self.pbar.update(1)\n",
    "        return False\n",
    "\n",
    "    def after_training(self, model):\n",
    "        self.pbar.close()\n",
    "        return model\n",
    "\n",
    "\n",
    "model = xgb.XGBClassifier(**params, callbacks=[TqdmCallback(params['n_estimators'])])\n",
    "model.fit(train_x_modified, train_y_modified)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3786ac902db5da9c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Submission"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "315df6859a4d8e09"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "473820bb35d69d97"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_values = pd.read_csv(f'{DATA_DIR}/test_values.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f46347f6248c1cf",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce4a5e485444195c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get geo level columns\n",
    "geo_test = test_values[geo_level_columns]\n",
    "\n",
    "# Label encoding\n",
    "geo_test.loc[:, \"geo_level_1_id\"] = label_encoder_1.transform(geo_test[\"geo_level_1_id\"])\n",
    "geo_test.loc[:, \"geo_level_2_id\"] = label_encoder_2.transform(geo_test[\"geo_level_2_id\"])\n",
    "geo_test.loc[:, \"geo_level_3_id\"] = label_encoder_3.transform(geo_test[\"geo_level_3_id\"])\n",
    "\n",
    "# Auto encoding\n",
    "geo_submission_tensor = torch.from_numpy(np.array(geo_test)).type(torch.long).to(DEVICE)\n",
    "geo_test = pd.DataFrame(encoder(geo_submission_tensor).detach().cpu().numpy())\n",
    "test_values_modified = pd.concat([test_values, geo_test], axis=1)\n",
    "\n",
    "# Remove low count geo id\n",
    "test_values_modified.loc[:, geo_level_columns] = remove_low_count.transform(\n",
    "    test_values_modified[geo_level_columns])\n",
    "\n",
    "# One-hot encoding\n",
    "test_encoded_categorical = one_hot_encoder.fit_transform(\n",
    "    test_values_modified[categorical_columns])\n",
    "test_encoded_categorical = pd.DataFrame(\n",
    "    test_encoded_categorical,\n",
    "    columns=one_hot_encoder.get_feature_names_out(categorical_columns)\n",
    ")\n",
    "test_values_modified.drop(categorical_columns, axis=1, inplace=True)\n",
    "test_values_modified = pd.concat([test_values_modified, test_encoded_categorical], axis=1)\n",
    "\n",
    "# Drop building id\n",
    "test_values_modified.drop('building_id', axis=1, inplace=True)\n",
    "\n",
    "# Fix columns type\n",
    "test_values_modified.columns = test_values_modified.columns.astype(str)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "783686c7efa5d649",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9829b5737807df0f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sub_pred = model.predict(test_values_modified)\n",
    "sub_pred = pd.DataFrame(sub_pred)\n",
    "\n",
    "submission = pd.read_csv(f'{DATA_DIR}/submission_format.csv')\n",
    "submission['damage_grade'] = sub_pred + 1\n",
    "submission.to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7c19922a9d6688d",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
